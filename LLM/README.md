[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-24ddc0f5d75046c5622901739e7c5dd533143b0c8e959d652212380cedb1ea36.svg)](https://classroom.github.com/a/VuMU77PL)
# llm-llama2-training

This repository is an example how to fine-tune LLama 2 model with parameter efficient fine-tuning, a low rank approximation of matrix and tensor structures, a 4-bit quantization of tensors, a transformer based Reinforcement Learning (RL) and HuggingFace's Supervised Fine-tuning trainer.

Prior to all this, the script contains code for creating a synthetic Hungarian dataset for our LLama 2 model to fine-tune on, in order to create task specific datasets for a given user query to fine-tune LLMs on.
