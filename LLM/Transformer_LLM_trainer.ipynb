{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer-based NLP applications\n",
        "\n",
        "#### HuggingFace example, LLAMA2 7B finetune.\n",
        "<br/><br/>\n",
        "Jelen Jupyter notebook a Budapesti Műszaki és Gazdaságtudományi Egyetemen tartott \"Mélytanulás\" tantárgy segédanyagaként készült.\n",
        "A tantárgy honlapja: https://portal.vik.bme.hu/kepzes/targyak/VITMMA19\n",
        "\n",
        "A notebook bármely részének újra felhasználása, publikálása csak a szerzők írásos beleegyezése esetén megengedett.\n",
        "***\n",
        "\n",
        "This Jupyter notebook was created as part of the \"Deep learning / VITMMA19\" class at Budapest University of Technology and Economics, Hungary, https://portal.vik.bme.hu/kepzes/targyak/VITMMA19.\n",
        "\n",
        "Any re-use or publication of any part of the notebook is only allowed with the written consent of the authors.\n",
        "\n",
        "2023 (c) Lívia Ónozó\n",
        "<br/><br/>\n",
        "## Remarks\n",
        "\n",
        "0. Make sure you have access to llama-7b-chat-hf modell (https://huggingface.co/meta-llama/Llama-2-7b-chat-hf) and OpenAI (https://openai.com/pricing) they provide $5 in free credit that can be used during your first 3 months\n",
        "1. Go to Runtime -> change runtime type\n",
        "2. Insert your OpenAI API key (if missing -> go to 6.)\n",
        "3. In \"prompt\" you can describe your own model you want to build\n",
        "4. Temperature can be between 0 and 1 (high=creative, low=precise)\n",
        "5. Prepared database can be found in the repo, too\n",
        "6. Modeling: You can also change the `model_name` if you want to fine-tune a different model. Go through the hyperparams!\n",
        "7. Saving and merging the model\n",
        "8. Inference\n"
      ],
      "metadata": {
        "id": "T8dCpp2rpVEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Setup: Install libraries\n",
        "\n",
        "Check out the following packages:\n",
        "- PEFT: https://huggingface.co/blog/peft\n",
        "- AutoClasses: https://huggingface.co/docs/transformers/model_doc/auto\n",
        "- LoRA: https://huggingface.co/docs/peft/conceptual_guides/lora\n",
        "- SFTTrainer: https://huggingface.co/docs/trl/sft_trainer\n",
        "- Bitsandbytes: https://huggingface.co/docs/transformers/main_classes/quantization"
      ],
      "metadata": {
        "id": "AbrFgrhG_xYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28.1\n",
        "import openai"
      ],
      "metadata": {
        "id": "zuL2UaqlsmBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Just in case of error while \"pip install trl\" uncomment the first 2 code lines and run all 3 lines. Otherwise run only the pip install\n",
        "#import locale\n",
        "#locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
      ],
      "metadata": {
        "id": "ChyrMwhNZrKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer\n",
        "#from peft.auto import AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "lPG7wEPetFx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = \"your API key\""
      ],
      "metadata": {
        "id": "WFlK_SarUsXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Data generation step"
      ],
      "metadata": {
        "id": "Way3_PuPpIuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can write your prompt here. Make it as descriptive as possible!\n",
        "\n",
        "Then, choose the temperature (between 0 and 1) to use when generating data. Lower values are great for precise tasks, like writing code, whereas larger values are better for creative tasks, like writing stories.\n",
        "\n",
        "Finally, choose how many examples you want to generate. The more you generate, a) the longer it takes and b) the more expensive data generation will be. But generally, more examples will lead to a higher-quality model. 100 is usually the minimum to start."
      ],
      "metadata": {
        "id": "lY-3DvlIpVSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"A model that takes in a puzzle-like reasoning-heavy question in Hungarian, and responds with a well-reasoned, step-by-step thought out response in Hungarian.\"\n",
        "temperature = .4\n",
        "number_of_examples = 10"
      ],
      "metadata": {
        "id": "R7WKZyxtpUPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this to generate the dataset."
      ],
      "metadata": {
        "id": "1snNou5PrIci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_example(prompt, prev_examples, temperature=.5):\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a \\\n",
        "            high-level description of the model we want to train, and from that, you will generate data samples, each with a \\\n",
        "            prompt/response pair.\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    if len(prev_examples) > 0:\n",
        "        if len(prev_examples) > 10:\n",
        "            prev_examples = random.sample(prev_examples, 5)\n",
        "        for example in prev_examples:\n",
        "            messages.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": example\n",
        "            })\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=386,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message['content']\n",
        "\n",
        "\n",
        "prev_examples = []\n",
        "for i in range(number_of_examples):\n",
        "    print(f'Generating example {i}')\n",
        "    example = generate_example(prompt, prev_examples, temperature)\n",
        "    prev_examples.append(example)\n",
        "    print(example)"
      ],
      "metadata": {
        "id": "oPr-28x6BI3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to generate a system message. Feel free to modify it, to comply with your taks!"
      ],
      "metadata": {
        "id": "KC6iJzXjugJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = 'Given a puzzle-like reasoning-heavy question in English, \\\n",
        "you will respond with a well-reasoned, step-by-step thought out response in Hungarian.'"
      ],
      "metadata": {
        "id": "SdWMFmnqONG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put examples into a dataframe and turn them into a final pair of datasets.\n",
        "\n",
        "We initialize lists to store prompts and responses. Then parse out them from examples.\n",
        "\n",
        "Also, removing duplicates, in case GPT was lazy to generate new pairs."
      ],
      "metadata": {
        "id": "G6BqZ-hjseBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = []\n",
        "responses = []\n",
        "\n",
        "for example in prev_examples:\n",
        "  try:\n",
        "    split_example = example.split('-----------')\n",
        "    if split_example[1] and split_example[3]:\n",
        "      prompts.append(split_example[1].strip())\n",
        "      responses.append(split_example[3].strip())\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'prompt': prompts,\n",
        "    'response': responses\n",
        "})\n",
        "\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "print('There are ' + str(len(df)) + ' successfully-generated examples. Here are the first few:')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "7CEdkYeRsdmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I you missed the first part, you can also use prepared dataset from the repository. Clone the repo,  find the llm_trainer_db.csv in the data folder and upload to Colab. Now you can read it as a pandas DataFrame"
      ],
      "metadata": {
        "id": "_giOw7IaSnyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/llm_trainer_db.csv')\n",
        "len(df)"
      ],
      "metadata": {
        "id": "2tAi2AFySm1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into train and test sets."
      ],
      "metadata": {
        "id": "A-8dt5qqtpgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets, with 90% in the train set\n",
        "train_df = df.sample(frac=0.8, random_state=42)\n",
        "test_df = df.drop(train_df.index)\n",
        "\n",
        "# Save the dataframes to .jsonl files\n",
        "train_df.to_json('train.jsonl', orient='records', lines=True)\n",
        "test_df.to_json('test.jsonl', orient='records', lines=True)"
      ],
      "metadata": {
        "id": "GFPEn1omtrXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Modeling\n",
        "\n",
        "## Load datasets"
      ],
      "metadata": {
        "id": "moVo0led-6tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = load_dataset('json', data_files='/content/train.jsonl', split=\"train\")\n",
        "valid_dataset = load_dataset('json', data_files='/content/test.jsonl', split=\"train\")\n",
        "\n",
        "train_dataset_mapped = train_dataset.map(lambda examples: {'text': [f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)\n",
        "valid_dataset_mapped = valid_dataset.map(lambda examples: {'text': [f'[INST] <<SYS>>\\n{system_message.strip()}\\n<</SYS>>\\n\\n' + prompt + ' [/INST] ' + response for prompt, response in zip(examples['prompt'], examples['response'])]}, batched=True)"
      ],
      "metadata": {
        "id": "1LU3j8oDgvze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Hyperparameters"
      ],
      "metadata": {
        "id": "Uy6ZC2ASpsyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"NousResearch/llama-2-7b-chat-hf\"\n",
        "dataset_name = \"/content/train.jsonl\"\n",
        "new_model = \"llama-2-7b-custom\"\n",
        "\n",
        "lora_r = 16\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "\n",
        "use_4bit = True\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "use_nested_quant = False\n",
        "output_dir = \"./results\"\n",
        "num_train_epochs = 3\n",
        "fp16 = False # test True\n",
        "bf16 = False\n",
        "per_device_train_batch_size = 4\n",
        "per_device_eval_batch_size = 4\n",
        "gradient_accumulation_steps = 1\n",
        "gradient_checkpointing = True\n",
        "max_grad_norm = 0.3\n",
        "learning_rate = 2e-4\n",
        "weight_decay = 0.001\n",
        "optim = \"paged_adamw_32bit\"\n",
        "lr_scheduler_type = \"constant\"\n",
        "max_steps = -1\n",
        "warmup_ratio = 0.03\n",
        "group_by_length = True\n",
        "save_steps = 25\n",
        "logging_steps = 5\n",
        "max_seq_length = None\n",
        "packing = False\n",
        "device_map = {\"\": 0}"
      ],
      "metadata": {
        "id": "bqfbhUZI-4c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Set training parameters and supervised fine-tuning parameters"
      ],
      "metadata": {
        "id": "F-J5p5KS_MZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit, #\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=device_map\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    fp16=fp16,\n",
        "    bf16=bf16,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=group_by_length,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    report_to=\"all\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=5\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset_mapped,\n",
        "    eval_dataset=valid_dataset_mapped,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=packing,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.model.save_pretrained(new_model)"
      ],
      "metadata": {
        "id": "qf1qxbiF-x6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test the model"
      ],
      "metadata": {
        "id": "vAdBIuMIp4Pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.set_verbosity(logging.CRITICAL)\n",
        "prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\nWrite a function which accepts a string and returns a new string with only capital letters. [/INST]\" # replace the command here with something relevant to your task\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=150)\n",
        "result = pipe(prompt)\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "pauavto53oEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.set_verbosity(logging.CRITICAL)\n",
        "prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\nWrite the exact recipe for making the Hungarian horn cake, kürtöskalács, but also add cardamom to the ingredients. [/INST]\" # replace the command here with something relevant to your task\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(prompt)\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "zPQwwA_1isSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.set_verbosity(logging.CRITICAL)\n",
        "prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\nAdd meg a kürtöskalács ekészítésének pontos receptjét és add hozzá a kardamomot az összetevőkhöz. [/INST]\" # replace the command here with something relevant to your task\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(prompt)\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "JpRf6npEj6vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.set_verbosity(logging.CRITICAL)\n",
        "prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\nMi az oka annak, hogy Magyarországon még nem vezették be az eurót, mint fizetőeszközt? Csak egy okot adj meg!\\n [/INST]\" # replace the command here with something relevant to your task\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(prompt)\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "z-zFM8PllFzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.set_verbosity(logging.CRITICAL)\n",
        "prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\nWhy the euro has not yet been introduced as a currency in Hungary? Please name just one reason. \\n [/INST]\" # replace the command here with something relevant to your task\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(prompt)\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "Eg-EFNKW0Jkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Number of trainable parameters\n",
        "\n",
        "LoRa (Low-Rank Adaptation) is a technique used to reduce the number of trainable parameters in a neural network model, particularly in the context of fine-tuning or transfer learning. It works by approximating the weight matrices with lower-rank matrices, which have fewer parameters. Here's how LoRa decreases the number of trainable parameters:\n",
        "1. Weight Matrix Decomposition: LoRa decomposes the original (typically low rank) weight matrices of the neural network, that often contain a large number of parameters, and LoRa aims to reduce this parameter count.\n",
        "2. Low-Rank Approximation: The decomposition technique used in LoRa approximates the original weight matrix with two (or more) smaller matrices.\n",
        "3. Parameter Sharing: In some cases, LoRa also employs parameter sharing, where the same low-rank matrices are used in multiple layers within the network. This further reduces the number of unique parameters that need to be trained.\n",
        "4. Fine-Tuning: After the low-rank decomposition is applied, the network is typically fine-tuned on the specific task for which it is being used. During fine-tuning, the model adjusts the reduced number of parameters to adapt to the new task."
      ],
      "metadata": {
        "id": "-a4vTzGl7gzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_trainable_parameters(model):\n",
        "  trainable_params = 0\n",
        "  all_param = 0\n",
        "  for _, param in model.named_parameters():\n",
        "    all_param += param.numel()\n",
        "    if param.requires_grad:\n",
        "      trainable_params += param.numel()\n",
        "\n",
        "  return trainable_params, all_param\n",
        "\n",
        "trainable, all = count_trainable_parameters(model)\n",
        "print('Traiable parameters as percentage of all parameters: ', 100*trainable / all)\n",
        "\n",
        "\n",
        "with open(\"llm_task_04.txt\", \"w\") as file:\n",
        "    file.write(f\"{trainable}\\n\")\n",
        "    file.write(f\"{all}\\n\")\n",
        "    file.write(f\"{100*trainable / all}\\n\")\n",
        "    file.write(f\"{model._modules['model'].layers[0].self_attn.q_proj.lora_A['default'].out_features}\")"
      ],
      "metadata": {
        "id": "YUbxSe94KC9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Run Inference\n",
        "\n",
        "Replace the command here with something relevant to your task"
      ],
      "metadata": {
        "id": "F6fux9om_c4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\nWrite about the effect of the Habsburg occupation on Hungary. [/INST]\" # replace the command here with something relevant to your task\n",
        "num_new_tokens = 100\n",
        "num_prompt_tokens = len(tokenizer(prompt)['input_ids'])\n",
        "max_length = num_prompt_tokens + num_new_tokens\n",
        "\n",
        "gen = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=max_length)\n",
        "result = gen(prompt)\n",
        "print(result[0]['generated_text'].replace(prompt, ''))"
      ],
      "metadata": {
        "id": "4TXyfWKvGrso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"[INST] <<SYS>>\\n{system_message}\\n<</SYS>>\\n\\nWrite your favourite fact about the Hungairan people. [/INST]\"\n",
        "num_prompt_tokens = len(tokenizer(prompt)['input_ids'])\n",
        "max_length = num_prompt_tokens + num_new_tokens\n",
        "\n",
        "gen = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=max_length)\n",
        "result = gen(prompt)\n",
        "print(result[0]['generated_text'].replace(prompt, ''))"
      ],
      "metadata": {
        "id": "6oRIHjFDlvx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"[INST] <<SYS>> You are a helpful, respectful and honest sentiment analysis assistant. \\\n",
        "And you are supposed to classify the sentiment of the user's message into one of the following categories: \\\n",
        "'positive', 'negative' or 'neutral'. The answer should be only one word.<</SYS>>\\\n",
        "Sentence: Magyarország elvesztette területei nagyrészét a trianoni békeszerződés aláírásával. [/INST] Sentiment:\"\n",
        "num_new_tokens = 3\n",
        "num_prompt_tokens = len(tokenizer(prompt)['input_ids'])\n",
        "max_length = num_prompt_tokens + num_new_tokens\n",
        "\n",
        "gen = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=max_length)\n",
        "result1 = gen(prompt)\n",
        "print(result1[0]['generated_text'].replace(prompt, ''))"
      ],
      "metadata": {
        "id": "Qp7d21G1IB74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"[INST] <<SYS>> You are a helpful, respectful and honest sentiment analysis assistant. \\\n",
        "And you are supposed to classify the sentiment of the user's message into one of the following categories: \\\n",
        "'positive', 'negative' or 'neutral'. The answer should be only one word.<</SYS>>\\\n",
        "Sentence: A magyarok nem humorosak. [/INST] Sentiment:\"\n",
        "num_new_tokens = 3\n",
        "num_prompt_tokens = len(tokenizer(prompt)['input_ids'])\n",
        "max_length = num_prompt_tokens + num_new_tokens\n",
        "\n",
        "gen = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=max_length)\n",
        "result2 = gen(prompt)\n",
        "print(result2[0]['generated_text'].replace(prompt, ''))"
      ],
      "metadata": {
        "id": "7hxQ_Ero2IJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"[INST] <<SYS>> You are a helpful, respectful and honest sentiment analysis assistant. \\\n",
        "And you are supposed to classify the sentiment of the user's message into one of the following categories: \\\n",
        "'positive', 'negative' or 'neutral'. The answer should be only one word.<</SYS>>\\\n",
        "Sentence: Világűröm vagy, s mélytengerem. [/INST] Sentiment:\"\n",
        "num_new_tokens = 3\n",
        "num_prompt_tokens = len(tokenizer(prompt)['input_ids'])\n",
        "max_length = num_prompt_tokens + num_new_tokens\n",
        "\n",
        "gen = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=max_length)\n",
        "result3 = gen(prompt)\n",
        "print(result3[0]['generated_text'].replace(prompt, ''))"
      ],
      "metadata": {
        "id": "wBRIsBmeNgPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"[INST] <<SYS>> You are a helpful, respectful and honest sentiment analysis assistant. \\\n",
        "And you are supposed to classify the sentiment of the user's message into one of the following categories: \\\n",
        "'positive', 'negative' or 'neutral'. The answer should be only one word.<</SYS>>\\\n",
        "Sentence: Leveleket fúj a viharos szél a komor utcán. [/INST] Sentiment:\"\n",
        "num_new_tokens = 3\n",
        "num_prompt_tokens = len(tokenizer(prompt)['input_ids'])\n",
        "max_length = num_prompt_tokens + num_new_tokens\n",
        "\n",
        "gen = pipeline('text-generation', model=model, tokenizer=tokenizer, max_length=max_length)\n",
        "result4 = gen(prompt)\n",
        "print(result4[0]['generated_text'].replace(prompt, ''))"
      ],
      "metadata": {
        "id": "qYC1EMe7YbZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your tasks will be the following:\n",
        "---\n",
        "\n",
        "1. Create your own dataset\n",
        "\n",
        "Create a file with the name llm_task_01.txt. This first line should containt the length of the training dataset, the second line should be the legnth of the test dataset.\n",
        "Before you save it, make sure that your dataset contains at least 10 question-answer pairs!\n",
        "\n",
        "2 points.\n",
        "\n",
        "---\n",
        "2. Change LoRA rank\n",
        "\n",
        "We have talked about the low-rank adaptation to make fine-tuning more efficient. LoRA projects the weight updates into two smaller matrices (called update matrices) through low-rank decomposition. These new matrices will be trained to adapt to the new data while keeping the original weights untouched. (Berfore merging them with the updates, of course.)\n",
        "\n",
        "\n",
        "Train a LLama2 7b chat model on Hungarian data, but this time the LoRA rank should be 4. For this, you will have to change the configuration of LoRA.\n",
        "If the training is ready, dump the model object.\n",
        "\n",
        "Hint: use the str(model) to serialize the object\n",
        "```\n",
        "with open(\"llm_task_02.txt\", \"w\") as file:\n",
        "    file.write(str(model))\n",
        "```\n",
        "8 points.\n",
        "\n",
        "---\n",
        "3. Reduce validation loss\n",
        "\n",
        "Make modifications on the training, on even on the training dataset in order to decrease validation loss.\n",
        "\n",
        "Save the training history into the file llm_task_03.txt, in the following way:\n",
        "```\n",
        "training_history = trainer.state.log_history[-3:]\n",
        "\n",
        "with open(\"llm_task_03.txt\", \"w\") as file:\n",
        "  file.write(f\"{training_history[0]}\\n\")\n",
        "  file.write(f\"{training_history[1]}\\n\")\n",
        "  file.write(f\"{training_history[2]}\")\n",
        "```\n",
        "\n",
        "\n",
        "15 points.\n",
        "\n",
        "---\n",
        "4. Change the configuration\n",
        "\n",
        "Change the hyperparameters on a way that the trainable parameters (compared to all parameters of the original model) will be less than 0.03 percent!\n",
        "\n",
        "Save the correspoinding parameters using the following code:\n",
        "\n",
        "```\n",
        "with open(\"llm_task_04.txt\", \"w\") as file:\n",
        "    file.write(f\"{trainable}\\n\")\n",
        "    file.write(f\"{all}\\n\")\n",
        "    file.write(f\"{100*trainable / all}\\n\")\n",
        "    file.write(f\"{model._modules['model'].layers[0].self_attn.q_proj.lora_A['default'].out_features}\")\n",
        "```\n",
        "\n",
        "25 points.\n",
        "\n",
        "---\n",
        "\n",
        "5. Train your model for sequence classification\n",
        "\n",
        "Train a model with **AutoModelForSequenceClassification** on Hungarian data (you can use the one we have used previously, but you can also choose a different dataset).\n",
        "<br/><br/>\n",
        "So you need to align your model with the new AutoModel class as well as the hyperparameters. On the other hand you will also need to change the prompt (but keep the [INST] <\\<SYS>>\\n{system_message}\\n<\\</SYS>> [/INST] structure!\n",
        "\n",
        "This is a multi-label classification problem, where you need to classify the sentences into 5 categories:\n",
        "After you trained your model in the above described way, it will need to answer 3 questions, and the answer should be one of the following ansvers: \"Általános\", \"Gazdaság\", \"Érzelem\", \"Sport\", \"Tudomány\"\n",
        "<br/><br/>\n",
        "\n",
        "Please report 3 files:\n",
        "- The _model_ object and the prompt in a txt file, named llm_task_05.txt,\n",
        "- The 3 sentences and the corresponding labels in a csv file, named llm_task_05.csv. Include a header, where the columns are: sentences, labels. So this should be a 4 by 2 matrix:\n",
        "\n",
        "| sentences | labels |\n",
        "| --- | --- |\n",
        "| sentence1 | label1 |\n",
        "| sentence2 | label2 |\n",
        "| sentence3 | label3 |\n",
        "\n",
        "- Your ipynb file containing all the calculations, saved as llm_task_05.ipynb\n",
        "\n",
        "\n",
        "50 points.\n",
        "\n",
        "---\n",
        "<br/><br/>\n",
        "All files must be saved directly into the git repository you submit. You may want to test your results before submitting, so here are the codes you will need for them: get the binary of the test from the main repo, modify the persmission so you can execute the file, and run. Alternatively, you can clone the repository and upload them to Colab. In this case check the file paths!"
      ],
      "metadata": {
        "id": "mAZxgu53fs3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get and run the tests"
      ],
      "metadata": {
        "id": "L7uQ5dRrudYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/BME-SmartLab-VITMMA19/llm-llama2-training/raw/main/llm_task_01\n",
        "!wget https://github.com/BME-SmartLab-VITMMA19/llm-llama2-training/raw/main/llm_task_02\n",
        "!wget https://github.com/BME-SmartLab-VITMMA19/llm-llama2-training/raw/main/llm_task_03\n",
        "!wget https://github.com/BME-SmartLab-VITMMA19/llm-llama2-training/raw/main/llm_task_04\n",
        "!wget https://github.com/BME-SmartLab-VITMMA19/llm-llama2-training/raw/main/llm_task_05\n",
        "\n",
        "!chmod u+x llm_task_01\n",
        "!chmod u+x llm_task_02\n",
        "!chmod u+x llm_task_03\n",
        "!chmod u+x llm_task_04\n",
        "!chmod u+x llm_task_05"
      ],
      "metadata": {
        "id": "WCwRpLjL1o8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./llm_task_01\n",
        "!./llm_task_02\n",
        "!./llm_task_03\n",
        "!./llm_task_04\n",
        "!./llm_task_05"
      ],
      "metadata": {
        "id": "5ecHnfxj2AWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Merge the model and store in Google Drive"
      ],
      "metadata": {
        "id": "Ko6UkINu_qSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Merge and save the fine-tuned model\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/llama-2-7b-custom\"\n",
        "\n",
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "# Save the merged model\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ],
      "metadata": {
        "id": "AgKCL7fTyp9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Load a fine-tuned model from Drive and run inference"
      ],
      "metadata": {
        "id": "do-dFdE5zWGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "model_path = \"/content/drive/MyDrive/llama-2-7b-custom\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "xg6nHPsLzMw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Mennyi 2 + 2?\"  # change to your desired prompt\n",
        "gen = pipeline('text-generation', model=model, tokenizer=tokenizer, max_new_tokens=20)\n",
        "result = gen(prompt)\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "fBK2aE2KzZ05"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Ko6UkINu_qSx",
        "do-dFdE5zWGO"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}